# llamacpp-speculative
llama-cpp-python server with speculative decoding
